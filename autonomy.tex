%================================================================================
%       Safety Critical Systems Club - Data Safety Initiative Working Group
%================================================================================
%                       DDDD    SSSS  IIIII  W   W   GGGG
%                       D   D  S        I    W   W  G   
%                       D   D   SSS     I    W W W  G  GG
%                       D   D      S    I    WW WW  G   G
%                       DDDD   SSSS   IIIII  W   W   GGG
%================================================================================
%               Data Safety Guidance Document - LaTeX Source File
%================================================================================
%
% Description:
%   AI and autonomy section.
%
%================================================================================
\section{AI and autonomy (Informative)} \label{bkm:autonomy}\index{AI|textbf}\index{Autonomy|textbf}

\cbstart
%
%\dsiwgSectionQuote{The first rule of any technology used in a business is that automation applied to an efficient operation will magnify the efficiency.
%  The second is that automation applied to an inefficient operation will magnify the inefficiency.}{Bill Gates}\todo{Pick a suitable quote}
%
\dsiwgSectionQuote{The potential benefits of artificial intelligence are huge, so are the dangers.}{Dave Waters}
%
%
%
\subsection{Job Displacement}\index{AI!Job displacement}
AI and automation can lead to the displacement of jobs, as machines can perform some tasks more efficiently than humans. This can have significant socioeconomic implications, including increased unemployment and wage stagnation in affected industries. The International Monetary Fund, (IMF), in January 2024, gave an estimate that AI will impact 40\% of jobs globally, and 60\% of highly skilled work, (ref 1). The report balances the already known complementary nature of AI to human work against the likely detrimental aspects. Unusually for technological advances, AI is expected to impact high-skilled jobs to a greater extent than manual skills. The report states, “advanced economies face greater risks from AI—but also more opportunities to leverage its benefits”. Labour requirements could significantly reduce, wages could be lower and of course jobs will disappear.  Managing Director of the IMF, Kristalina Georgieva, said that “AI will likely worsen overall inequality , a troubling trend that policymakers must proactively address to prevent the technology from further stoking social tensions.” (Ref 2). How economists are changing the standard growth view of economics and how engineering and other businesses can engage with such  developing problems is discussed in an article in the SCSC Newsletter edition 2024 (Ref 3). The data used to determine the impact of an AI system, especially when it affects currently underdeveloped nations must ensure that the social tension Georgieva refers to does not reach a breaking point. Like well digging, rail construction and careful dam placement, AI technology could raise many millions out of poverty if it is applied to benefit life on earth.

\begin{itemize}
\item Ref 1: International Monetary Fund, Staff Discussion Notes, “Gen-AI: Artificial Intelligence and the Future of Work”, Mauro Cazzaniga et al, (14/01/2024).
\item Ref 2: Western Daily Press, “AI to hit 40\% of jobs and will worsen inequality” Holly Williams, (16/01/2024) 
\item Ref 3: Engineering, Ethics and ‘Doughnut’ Economics, Nicholas Hales,  Safety Critical Systems Club Newsletter Feb 24 edition. Vol 32 Nos 2 SCSC-193
\end{itemize}

\subsection{Bias and Discrimination}\index{AI!Bias}\index{AI!Discrimination}
AI systems can inherit biases present in their training data, leading to discriminatory practices. For instance, facial recognition software has been shown to have higher error rates for people of certain racial and ethnic groups. At a very basic level, slight imperfections in coatings for car registration plates, easily ignored by humans, has made some number plates unrecognisable to automatic Number Plate Recognition cameras at car parks. This can be easily missed in training the system since it would have been difficult to anticipate the degree of training a system needs to recognise every possible imperfect state of a number plate. In this case advances are made from experience but there are many situations in which day one of a system’s deployment has to be near perfect. Training data sets should be vetted by committees, as safety usually is and should be always. That technique is more likely to bring out overlooked areas, obvious to someone on the committee but totally missed by others. Testing of developed systems should also cover the full range of possibilities, whether or not those involved in the development provide assurances, as assurances can hide failings.

\subsection{Loss of Human Skills}\index{Autonomy!Loss of human skills}
With AI taking over tasks such as navigation or memory-dependent activities, there is a risk that humans may lose certain skills that are not used regularly, potentially reducing cognitive abilities over time.  However, the opposite is also true, because driven by the threat of automation, many people could turn to artisan skills to create unique or less ubiquitous products providing an element of appealing individuality. This could turn supermarket check-out staff into, for instance entrepreneurial upholsterers, when and where all check-outs have become automated, or cosmetic developers. Regardless of whether or not skills are lost, it is certain that in order to verify and validate data intensive artificial intelligence systems someone will be required to maintain those basic skills to provide the assurance. To make a simple analogy, one cannot validate a French grammar correcting AI system if one does not speak French fluently.

\subsection{Security Risks}\index{AI!Security risks}
AI can be used to develop sophisticated cyber-attacks, and AI systems themselves can be vulnerable to such attacks. The integration of AI into critical infrastructure heightens the potential impact of these risks. The issue of autonomous vehicles in military applications has been discussed in an SCSC newsletter, (Ref 1) wherein a quote from a paper, “Formal Verification of Ethical Choices in an Autonomous System” (Ref 2), the following was written, “All participants in society are required to follow specific regulations and laws. An autonomous system cannot be an exception”. Given this as an accepted  principle it is then incumbent on engineers to design systems which follow ethical regulations. This presents difficulties and it demands that the most sophisticated sensors be employed on killer robots when decisions will need to be made as to whether or not to engage in an action which may result in death or excessive damage. Such examples are, firing a missile at a market square or bridge where known enemies are observed. At what point does the death of others not engaged in wrong doing become acceptable, known as collateral damage. Although it does not involve many robots, the conflict in the Gaza Strip is a prime example of this issue of what collateral damage is acceptable. That issue will be for the courts of justice to decide. Again it is important to stress that  the data used to train systems covers all possibilities, just as in human v  human conflict. If the machine is not programmed to make the decision, the machine must request clarification from human controllers. The issue is that it will almost certainly prove impossible to develop military killer systems that can be programmed with sufficient data to make every decision required in all circumstances without needing to consult humans, though direct accurate hits on isolated targets with little risk to the combatants of the targeting force is likely to remain an acceptable usage. But it would be sad if the governments of the world end up pitting machines they don’t understand against each other with all the horrendous possible consequences.
\begin{itemize}
\item Ref 1: “Autonomy’s Troubling Questions”, Nicholas Hales The Safety Critical Systems Club Newsletter Volume 27 Number 1, Winter 2018-19.
\item Ref 2: “Formal Verification of Ethical Choices in Autonomous Systems”, Robotics and Autonomous Systems Voll 77 (2015). L Dennis, M Fisher, M. Slavkovik and M Webster.
\end{itemize}

\subsection{Privacy Erosion}\index{AI!Privacy erosion}
AI's ability to analyse vast quantities of personal data can lead to erosion of privacy. For instance, AI can be used to make highly accurate predictions about individuals’ behaviours, preferences, and even future actions. To a limited extent this is already operational and desired by police forces wanting to identify faces and even the gait of wanted persons. The usual claim is that “If you are not doing anything wrong you have nothing to fear”. But many democracies are turning against the principle of eroding the human right to not  be constantly under surveillance or have already done so, trusting in the goodwill of the majority of citizens and accepting some policing techniques AI makes available are too close to fascism of the left or right wings of politics. A reply that may make advocates of surveillance policing change is to recommend they put cameras on the stairs in their house leading up to their bedrooms, just to check the wife is not having an affair with a colleague or the milkman because “If he or she is not doing anything wrong, they have nothing to fear”. Such a statement applied in the domestic situation may make the advocate awre that not everybody is happy about being the watchful eye of government. Even when the nation is not fascistic the creation of the network of cameras across a nation does provide the environment for a takeover by fascist elements. The Metropolitan Police  of London are using live facial recognition cameras. (Ref 1). Their policy is  “LFR cameras are focused on a specific area; when people pass through that area their images are streamed directly to the Live Facial Recognition system. This system contains a watchlist: a list of offenders wanted by the police and/or the courts, or those who pose a risk of harm to themselves or others.”  They currently assert that “LFR is not a ubiquitous tool that uses lots of CCTV cameras from across London to track every person’s movements”.  But a change of government, laws or the Mayor of London could change that. The risk is in the data being held may be abused or false, (police personnel have lied to get convictions in the past), the ability of the cameras to differentiate faces or gaits and the potential expansion of the system.

\begin{itemize}
\item Ref 1: \raggedright{\href{https://www.met.police.uk/advice/advice-and-information/fr/facial-recognition-technology/#:~:text=LFR\%20cameras\%20are\%20focused\%20on,harm\%20to\%20themselves\%20or\%20others}{https://www.met.police.uk/advice/advice-and-information/fr/facial-recognition-technology/\#:\~: text=LFR\%20cameras\%20are\%20focused\%20on,harm\%20to\%20themselves\%20or\%20others}}
\end{itemize}

\subsection{Control, Autonomy, Ethical and Moral Considerations}\index{AI!Control, Autonomy, Ethical and Moral Considerations}
As AI systems become more autonomous, there is a risk that they may act in unforeseen ways that are not aligned with human intentions or may be manipulated to act against human interests. Also there are significant ethical questions around AI, including the morality of decisions made by AI systems, particularly in life-and-death situations such as in autonomous vehicles or military applications. Although mainly concerned with military questions  “The Troubling Aspects of Autonomy” (Ref 1) discusses the issues of control that arise for dangerous life-threatening situations and is pertinent to non-military systems.
\begin{itemize}
\item Ref 1: “Autonomy’s Troubling Questions”, Nicholas Hales The Safety Critical Systems Club Newsletter Volume 27 Number 1, Winter 2018-19.
\end{itemize}

\subsection{Economic Inequality}\index{AI!Economic inequality}
The benefits of AI may accrue disproportionately to those who own the technology, potentially exacerbating economic inequality. Companies and nations that can invest in AI could gain significant economic advantages, leaving others behind. The likely beneficiaries are the developed economies. 40\% globally will be affected according to an IMF discussion paper and 60\% in the advanced technology world. Achim Steiner, Director of the Oxford Martin School, has said: “With new advances in the development of AI being made at a rapid pace, it is critical that as much importance is placed upon safety and ethical considerations as on technological progress”. In the fast world of competitive developments in AI which has now begun,  it is difficult to believe that the potential pitfalls are ever fully examined when, just as in more conventional engineering systems, the bottom line remains money gained through business success. Only regulation by governments and other authorities can be expected to be effective in limiting damage outside the bounds of AI systems. Some are now advocating a new form of economics known as Doughnut Economics as advocated by Professor Kate Raworth, Senior Teaching Associate at Oxford University’s Environmental Change Institute in her book on the subject, (Ref 1). A brief discussion of the some of the issues of such economics can be found in (Ref 2). There could be ways to support business in developing countries if AI tools are made available on smart phones at low cost to, for instance, African women’s business collectives. Data input and received from such applications will need to be safe and accurate if it is to benefit the less well-off of the world. Businesses must have good data to when using AI that provides reasonably accurate appraisals of outcomes from various actions. Ultimately, nobody should be left falling short of the social foundation that the Doughnut Economy declares the future ‘bottom line’ to be. In the Appendix to Raworth’s book, a table is given showing the issues that need addressing and declaring the sources. Such sources should be consulted for trusted data when new projects are developed. Typical statistic given would be an update to the books figures but similarly sourced, “Percentage of World population under-nourished = 11\% in years 2014-2016. 
\begin{itemize}
\item Ref 1: “Doughnut Economics – Seven ways to Think Like a 21st Century Economist” Professor Kate Raworth, Penguin Books, 2017 
\item Ref 2: Engineering, Ethics and ‘Doughnut’ Economics, Nicholas Hales,  Safety Critical Systems Club Newsletter Feb 24 edition. Vol 32 Nos 2 SCSC-193
\end{itemize}

\subsection{Dependency}\index{AI!Dependency}
Over-reliance on AI can lead to a lack of preparedness when systems fail. If critical infrastructure or services are AI-dependent, outages or malfunctions could have severe consequences. Simple growth of single point failure understanding applied to AI should mitigate a lot of the risk of loss of urgent data, like digital aircraft having three computer with a voting sub-system in overall control.

\subsection{Manipulation and Fake Content}\index{AI!Manipulation}\index{AI!Fake content}
AI can be used to create deep-fakes and synthetic media, which can be used to manipulate public opinion, perpetrate fraud, or spread misinformation. The Partnership for AI, (Ref 1),  have pursued the concept of there being a spectrum of harm from manipulated media. How this will all resolve is as yet unknown, the problem is in its relative infancy. With partners, a working definition arrived at is “any image or video with content or context edited along the “cheap fake” to “deepfake” spectrum (Data \& Society”) with the potential to mislead and cause harm” (Ref 2. A diagram and explanation of the spectrum can be found at Data and Society’s website (Ref 3). 12 Principles for labelling manipulated data can be found at the Partnership on AI (Ref 4) . As to whether any of this is enough, only time will tell. Much is seen in the news of demands for content providers to do better at eliminating bad data, be it manipulated or such things as sites encouraging suicide. However, it is worth noting that The Future of Humanity Institute (FHI), a research group within the Oxford Martin School, has joined the Partnership on AI and that organisation has as members many of the known big players in the data intensive media, Amazon, Apple, Google/DeepMind, Facebook, IBM and Microsoft. There goal is to formulate socially beneficial best practices for AI development. How much their belief in their achievements coincides with politicians requirements we must wait and see.  The Partnership for AI has produced a document “PAI’s responsible Practices for Synthetic Media”, (Ref 5), In which it is shown that current theory is that it is clear the best way to overcome fake media is by the organisations not trying to deceive anybody to have codes of practice. They seek to advance ethical and responsible behaviour. Synthetic Media, also known as generative media is defined as “..visual, auditory or multimodal content that has been generated or modified, commonly by artificial intelligence.”

It is accepted that legitimate use of such media may be for entertainment, art, satire, education and research to mention a few categories. Furthermore, techniques can be used legitimately or harmfully, there is no barrier. One recommendation for technology and infrastructure creators is “Aim to disclose in a manner that mitigates speculation about content, strives towards resilience to manipulation or forgery, is accurately applied and also when necessary, communicates uncertainty without furthering speculation”. The guide goes on to recommend practices for ‘Creators’ and ‘Distributors and Publishers’. Transparency and Disclosure are the main thrusts. It therefore appears that the legal definition for consumer risk will mirror goods purchase, Caveat Emptor, let the buyer beware, but with safeguards provided by companies wishing to take ethical stances.
\begin{itemize}
\item Ref 1: www.PartnershiponAI.org The Partnership on AI, Taking a Methodical Approach to Best Practices.
\item Ref 2: \href{https://datasociety.net/library/deepfakes-and-cheap-fakes/}
  {https://datasociety.net/library/deepfakes-and-cheap-fakes/}
\item Ref 3: \href{https://datasociety.net/wp-content/uploads/2019/09/Deep\_fakes\_spectrum.png}
  {https://datasociety.net/wp-content/uploads/2019/09/Deep\_fakes\_spectrum.png}
\item Ref 4: \href{https://partnershiponai.org/it-matters-how-platforms-label-manipulated-media-here-are-12-principles-designers-should-follow/}
  {https://partnershiponai.org/it-matters-how-platforms-label-manipulated-media-here-are-12- principles-designers-should-follow/}
\item Ref 5: \href{https://partnershiponai.org/wp-content/uploads/2023/02/PAI\_synthetic\_media\_framework.pdf}
  {https://partnershiponai.org/wp-content/uploads/2023/02/PAI\_synthetic\_media\_framework.pdf}
\end{itemize}

\subsection{Existential Risk}\index{AI!Existential risk}
In the long term, some theorists and scientists worry that the creation of super-intelligent AI could pose an existential risk to humanity if it becomes uncontrollable or its goals are misaligned with human values. This particular aspect of Data Safety with AI systems shows how important it is that the data used in AI systems reflects humanities values. The Doughnut Economics model uses, as the social foundation for humanity, twelve of the seventeen goals of the United Nations. Any AI system that is not trained, where relevant, using data that will ultimately achieve those goals, is almost certainly not an ethical system designed to align with the established values of the global humanitarian consciousness. Also many systems capable of destroying, such as  weapons, that have been granted autonomy, need to have been trained using the relevant data reflecting guidelines, in the military case, on conflict rules. Issues related to the military problems are discussed in Ref 1
\begin{itemize}
\item Ref 1:  “Autonomy’s Troubling Questions”, Nicholas Hales The Safety Critical Systems Club Newsletter Volume 27 Number 1, Winter 2018-19.
\end{itemize}

\subsection{An AI's view of the risks}
We asked an AI ``What sort of mistakes due to AI can be hard to detect?.'' The AI replied as follows:

\begin{aibox}
Artificial intelligence (AI) systems, while powerful and increasingly integral to many aspects of modern life, are not infallible. There are several types of mistakes or errors in AI systems that can be particularly hard to detect:

Subtle Data Bias: AI algorithms are often only as good as the data they are trained on. If this data contains biases, the AI can perpetuate or even amplify these biases. These biases can be subtle and deeply ingrained in the data, making them difficult to detect without thorough analysis.

Overfitting: This occurs when an AI model is too closely tailored to the specifics of its training data and fails to generalize well to new, unseen data. Detecting overfitting can be challenging because the model may perform exceptionally well on training data but poorly in real-world applications.

Black Box Models: Many advanced AI models, like deep neural networks, are often considered "black boxes" because their decision-making processes are not transparent or interpretable. This lack of transparency makes it difficult to understand why the AI made a certain decision and, consequently, to identify when it makes mistakes.

Contextual Misinterpretation: AI may not fully understand or consider the context in which data exists, leading to misinterpretations. These errors can be hard to detect because the AI's output may be logically consistent within the data it has, but contextually inappropriate.

Feedback Loops: In systems where AI output influences future input data, feedback loops can occur. For instance, in content recommendation algorithms, the system can get trapped in a loop of reinforcing its own biases, making it hard to detect errors in content filtering or recommendation.

Anomalous Data or Edge Cases: AI systems might struggle with data or situations that are significantly different from what they were trained on, known as edge cases. These anomalies can lead to errors that are hard to anticipate and detect because they occur outside the 'normal' operating parameters.

Errors in Complex Interactions: In systems where multiple AI models interact, errors can arise from the complex interplay between these systems. These errors can be particularly challenging to detect and diagnose because they may not arise from any single model but from the interaction between models.

Evolution of Language and Social Norms: AI systems trained on language data can become outdated as language and social norms evolve. Detecting when an AI system is no longer accurately interpreting or generating language according to contemporary standards can be difficult.

Misalignment with Human Values: AI might make decisions that are technically correct but misaligned with human ethics or values. These types of mistakes can be particularly hard to detect because they require nuanced understanding of human ethics, culture, and values.

To mitigate these issues, continuous monitoring, testing, diverse data sets, and incorporating explainability in AI systems are crucial. Additionally, involving human oversight and interdisciplinary expertise can help in detecting and correcting such hard-to-detect mistakes.
\end{aibox}
\cbend
