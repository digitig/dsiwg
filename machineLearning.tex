%================================================================================
%       Safety Critical Systems Club - Data Safety Initiative Working Group
%================================================================================
%                       DDDD    SSSS  IIIII  W   W   GGGG
%                       D   D  S        I    W   W  G   
%                       D   D   SSS     I    W W W  G  GG
%                       D   D      S    I    WW WW  G   G
%                       DDDD   SSSS   IIIII  W   W   GGG
%================================================================================
%               Data Safety Guidance Document - LaTeX Source File
%================================================================================
%
% Description:
%   Machine Learning section.
%
%================================================================================
\section{\Glsfmtshort{ai} and Machine Learning (Informative)} \label{bkm:MachineLearning}

\dsiwgSectionQuote{Learning never exhausts the mind.}{Leonardo da Vinci}


\subsection{Machine Learning Training Data}\index{Machine Learning Data|textbf}
A complete discussion about the assurance of machine learning /\gls{ai} algorithms is beyond the scope of this document.
However, as such algorithms are inherently data-driven, the Data Safety Guidance provides the following notes on good practice for data sets used for machine learning / Artificial Intelligence applications.

The data for real-world machine learning
(ML)
scenarios may be effectively infinite (for example, with self-driving cars, where the environment is largely unbounded) and can only be approximated by a finite number of scenarios.
Therefore it is crucial to have an extension process in place to extend the system when previously unknown situations are experienced. 
ISO~21448 \cite{citation:ISO21448} captures this process by introducing the term \gls{sotif} and requires the systematic handling of unknowns. Although specifically aimed at road vehicles, \gls{sotif} may be of interest to other domains.

For more general guidance on managing the safety of Autonomous Systems, please refer to the safety assurance objectives guidance produced by the SCSC \gls{saswg} \cite{citation:SASWG:Objectives}.

\subsection{Data Categories}
Three data categories are discussed: training data\index{Machine Learning Data} (the data used to train the machine learning model),
ML test data\index{Category!Data!Verification} (used to provide a measure the trained model’s \index{Accuracy Property}accuracy), and ML
validation data (used to independently verify the trained model).  

\subsection{Real-world Data}
It is important to obtain sufficient quantities of data for the training data. It is generally assumed that this is sufficiently representative of situations that could be encountered to allow the algorithm to generalise the appropriate behaviour for all encountered situations.  However, there are problems with real-life training data. Firstly, it is possible that rare, hazardous cases are never encountered during the capture of a data set (as those events have been engineered to be statistically unlikely) and are too dangerous, or expensive, to generate deliberately. Poor representation of rare cases in training data will result in a model that is insufficiently prepared to handle those important scenarios. Poor representation (or weighting) of rare cases in
ML test data or ML
validation data can result in error heuristics that do not provide adequate insight into the system’s performance\index{Performance, System} in rare situations. A second issue with real-world data is that amassing, understanding, verifying and validating it at scale is resource intensive.
Hence it is not just a case of more testing, a more cost-effective approach is needed. Some possible techniques are: artificial
ML
test data set generation, data fault injection and simulation. It may well be more practicable to use simulated training data during the training phase to give more control and avoid the cost and effort of real-world testing. The issue then is how realistic the simulated data (and the associated simulation) can be. Note that artificial data sets can also be used to increase the frequency of rare events, thus biasing the algorithm to managing those rare events effectively, but doing so, could impair the ``sunny day'' performance\index{Performance, System} of the system.

\subsection{Sensor Data}
In many cases sensors will be on-board and embedded (e.g.\ on an automated vehicle) and will be subject to many real-world influences. Sensor data may be changed or degraded by both internal influences, e.g.\ system environment temperature, and external factors e.g.\ humidity, ambient light, weather conditions such as snow or fog, and rarely encountered extremes such as lightning or flood. 
Sensors may also suffer degradation due to ageing and specific faults due to their positioning, construction or configuration. Sensors may need to be diverse to increase resilience and give an overall more reliable output. Realistic data variations, informed by an understanding of likely sensor failure modes, need to be incorporated in the training data set (c.f.\ the AoA sensor in the Boeing 737 MAX~8, discussed in \autoref{bkm:incacc:737max8}).
Sensor fusion can effectively create ``composite data'', e.g.\ from merging of Lidar, Radar and Camera data; this will have different (and possibly hard to predict) \index{Property!Data}properties compared to the individual sensors.

\subsection{Data Coverage}
Some notion of data coverage, that is, how well the training data covers the intended operational domain, is needed: this could be based on use of all known data sets, all known hazardous situations or some other measure. White box techniques involved in exercising all connected sensors or coverage of nodes in a neural network may have some value. Regardless, it will be essential that the training data covers all required safety test scenarios.

\subsection{Data Set Assurance}
There is a need to have assurance about the data sets themselves, including their configuration management. Assessment of the quality and suitability of the training data sets should be included in the safety argument.

The University of York has developed a method for the Assurance of Machine Learning for use in Autonomous Systems (AMLAS)
\cite{citation:machinelearning:amlas}.
The University's approach is intended to assist in the development of a compelling argument about a machine learning model,
to feed into a system safety case.

\subsection{Data Diversity}
The degree of data diversity is important. Common data items across data sets need to be avoided, especially across training and
ML
validation data. Common data may include data derived from the same sources, environments, simulators, tools, techniques or mathematical models.

\subsection{\index{Property!Data}Data Properties}
Bias, sufficiency and \index{Accuracy Property}accuracy are all concerns with the data. \index{Fidelity/Representation Property}Fidelity may also be a problem as modelling real-life sensor data is difficult.  Errors should be introduced for training: this may be by analytical means, by random, automated sampling of data to determine the degree of similarity between samples, or by means of deliberate insertion of known edge-cases. 
It is suggested that all the \index{Property!Data}data properties mentioned in this guidance document are considered.

\subsection{Hallucinations}
\label{bkm:MachineLearning:Hallucinations}\index{Hallucinations|textbf}
%
The human brain can see and interpret patterns in unexpected ways, for example seeing a shadow that
suggests a monster is waiting to pounce, or seeing an animal in a cloud formation%
\footnote{These are not strictly hallucinations in the medical sense, but serve to illustrate
the issue with \gls{ai} interpretation of data.}.
\Gls{ai} tools can similarly interpret data in novel and unplanned ways, perhaps seeing patterns that a
human would not perceive, or seeing shapes in noise. However with \gls{ai} tools, hallucinations can appear
in two different ways --- in the interpretation of the question or instruction to perform, as well as
in the interpretation of the source data.
The problem is exacerbated in the case of \gls{ai} ``chatbots'', where the output presentation in natural
language can make the response seem authoritative and result in it being trusted, as though the
\gls{ai} chatbot is an expert in the field under consideration \cite{citation:chatbotsterrible:website}. 

The complete eradication of hallucinations may not be possible, so it can be dangerous to rely fully
on the output of an \gls{ai} without verification. Biases in the training data are regarded as a
significant cause of hallucinations, so selection of training data should include consideration
of its quality --- whether it is diverse, balanced and well structured
\cite{citation:halucinations:website}. In addition, evaluation of the resultant behaviour should be
assessed by humans, to achieve ``reinforcement learning with human feedback''
\cite{citation:howchatgptworks:website}.
The results of the assessments can then be fed back into the model,
thus fine-tuning it for its specific application.

Note that fine-tuning will generally only be possible for a specific application and/or a specific
environment. As with all safety analyses, it is critical that these be clearly defined.

We asked two different \glspl{ai} to comment on the risks that hallucinations present to data safety. Their responses were as presented in the
following two boxes:

\begin{aibox}%ChatGPT
  \Gls{ai} hallucinations can have significant implications for data safety and overall trust in \gls{ai} systems. Here are some key concerns:

Misinformation and Disinformation: If an \gls{ai} model generates incorrect or misleading information (hallucinates), and this information is used without verification, it can lead to the spread of misinformation. In sensitive areas like healthcare, finance, or law, this could have serious consequences.

Decision-Making Risks: In scenarios where \gls{ai} is used to aid decision-making, hallucinations can lead to incorrect or risky decisions. For instance, if a financial \gls{ai} advises on investments based on hallucinated data, it could lead to substantial financial losses.

Trust and Reliability Issues: Frequent hallucinations can undermine the trust in \gls{ai} systems. Users and organizations may become reluctant to rely on \gls{ai} for critical tasks if there's a significant risk of the \gls{ai} generating false information.

Security Vulnerabilities: \gls{ai} hallucinations can be exploited in cybersecurity. For example, if an \gls{ai} system responsible for network security misinterprets data or hallucinates a threat, it could either overlook actual threats or allocate resources to non-existent threats.

Privacy Concerns: There’s a risk that an \gls{ai} model might inadvertently reveal or imply private information in its outputs, especially if it has been trained on sensitive datasets. While not a hallucination in the traditional sense, this is related to the broader issue of how \gls{ai} handles and interprets data.

Ethical and Legal Implications: \gls{ai} hallucinations can lead to ethical and legal challenges, especially if they result in harm or damage. Determining liability in cases where AI-generated misinformation leads to real-world consequences is complex.

Bias and Fairness: Hallucinations can also be a reflection of biases in the training data. An \gls{ai} might generate outputs that are unfairly biased against certain groups or individuals, leading to ethical concerns and potential legal issues.

To mitigate these risks, it's crucial to have robust validation and verification processes, use \gls{ai} in conjunction with human oversight, continually update and improve \gls{ai} models with diverse and accurate data, and maintain transparency about the limitations and capabilities of \gls{ai} systems.
\end{aibox}

\begin{aibox}%Google Bard
  The implications of hallucinations for data safety are complex and far-reaching, spanning aspects of privacy, security, and misinformation. Here's a breakdown of some key concerns:

Privacy:

False generation of personal data: \gls{ai} hallucinations can fabricate personal information (names, addresses, financial details) that doesn't actually exist, potentially violating privacy rights and harming individuals.
Exposure of sensitive data: If trained on sensitive data, hallucinations might leak this information unintentionally, even if not explicitly instructed to do so. This could include medical records, financial data, or other confidential information.
Security:

Creating vulnerabilities: Hallucinations might invent security flaws or exploits that don't actually exist, leading to wasted resources and potentially even vulnerabilities being introduced into real systems.
Misleading security assessments: If used for security testing, hallucinations could create false positives or negatives, providing inaccurate assessments of a system's security posture.
Misinformation:

Spreading false information: \gls{ai}-generated hallucinations can easily spread misinformation, as they appear convincing and authoritative even if factually incorrect. This can have significant implications for public discourse, trust in institutions, and even manipulation.
Evolving deepfakes: Hallucinations could be used to create even more sophisticated deepfakes, making it harder to distinguish between real and fake content, further eroding trust and potentially influencing elections or other high-stakes situations.
Additional considerations:

Impact on trust: As reliance on \gls{ai} grows, hallucinations can erode trust in its outputs, making it harder to distinguish reliable information from fabricated content.
Regulation and accountability: Establishing clear guidelines and accountability mechanisms for \gls{ai} systems prone to hallucinations is crucial to mitigate their risks.
Overall, addressing the implications of hallucinations for data safety requires a multi-pronged approach:

Developing robust and transparent \gls{ai} systems: Techniques like adversarial training and explainable \gls{ai} can help reduce hallucinations and improve transparency.
Implementing data governance and privacy measures: Strong data protection frameworks and responsible data handling practices are essential to minimize the risk of sensitive data exposure.
Promoting media literacy and critical thinking: Educating users to critically evaluate information, regardless of its source, is crucial in the age of \gls{ai}-generated content.
Establishing ethical guidelines and regulations: Clear ethical frameworks and regulations for \gls{ai} development and deployment are necessary to ensure responsible use and mitigate potential risks.
By acknowledging the dangers of hallucinations and taking proactive steps to address them, we can minimize their impact on data safety and build trust in \gls{ai}-powered technologies.
\end{aibox}

\subsection{Data Set Management}
Data sets will be large and complex and ensuring quality and \index{Consistency!Property}consistency will be difficult. Measures will have to be taken to control, manage and archive large amounts of data. Data sets should be subject to formal change control and monitoring. Tools will be needed to manage the data.

\subsection{Optimization}
What is an optimal size of a training or
ML
validation data set? A data set may contain many repeated or redundant values. If these do not add to the training outcome it is possible that some values may be removed. Also if there are ways in which the data set could be compressed or optimized then these should be considered to assist management and curation.

\subsection{Compliance}
A ``safety by compliance'' strategy for such data is difficult to implement as there are currently
few existing standards and little guidance for data in this area.
It is therefore difficult to quantify data sufficiency and as a result, no generally agreed ``ALARP'' principle for data.

It may be possible to demonstrate Data Safety through statistical methods,
such as when millions of representative miles have been driven.
However such approaches have generally been found insufficiently robust to support safety-critical systems in other domains
such as aviation, and so must be treated with caution.
The demonstration of compliance for the data is likely to require a compliance argument for the tools that produced the data,
the preservation of \index{Property!Data}data properties as described within this document, and an update process such as that specified in ISO 21448 (SOTIF)
\cite{citation:ISO21448}.

\subsection{References}
This appendix provides only a brief overview of the issues around the application of data safety techniques to machine learning. It is based upon information in the following documents, which should be consulted for further details:
\begin{itemize}
\item Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges \cite{citation:Ashmore2019}
\item Quantifying Data set \index{Property!Data}Properties for Systematic Artificial Neural Network Classifier Verification \cite{citation:Hond2020}
\item Safety Critical Integrity Assurance in Large Data sets \cite{citation:Sutherland2020}
\end{itemize}
